{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44ef8ce-fcdf-498a-afec-5b10633403fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Using cached unsloth-2025.6.2-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting unsloth_zoo>=2025.6.1 (from unsloth)\n",
      "  Using cached unsloth_zoo-2025.6.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from unsloth) (2.5.1)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from unsloth) (24.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Using cached tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets>=3.4.1 (from unsloth)\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from unsloth) (5.9.8)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.12/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /opt/conda/lib/python3.12/site-packages (from unsloth) (0.34.2)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
      "  Using cached trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from unsloth) (5.28.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.12/site-packages (from unsloth) (0.29.3)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from unsloth) (0.20.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from vllm) (2024.11.6)\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.12/site-packages (from vllm) (5.5.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (2.32.3)\n",
      "Collecting blake3 (from vllm)\n",
      "  Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting huggingface_hub (from unsloth)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.11)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from vllm) (3.9.5)\n",
      "Collecting openai>=1.52.0 (from vllm)\n",
      "  Using cached openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /opt/conda/lib/python3.12/site-packages (from vllm) (2.10.6)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from vllm) (11.1.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Using cached llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting outlines==0.1.11 (from vllm)\n",
      "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.19 (from vllm)\n",
      "  Using cached xgrammar-0.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/conda/lib/python3.12/site-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (3.18.0)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (26.3.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Using cached gguf-0.17.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Using cached mistral_common-1.6.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
      "  Using cached setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.1 (from vllm)\n",
      "  Using cached compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm)\n",
      "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.12/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /opt/conda/lib/python3.12/site-packages (from vllm) (0.24.0)\n",
      "Requirement already satisfied: python-json-logger in /opt/conda/lib/python3.12/site-packages (from vllm) (2.0.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from vllm) (1.15.2)\n",
      "Collecting ninja (from vllm)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.31.0)\n",
      "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
      "  Using cached opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Using cached numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached ray-2.47.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm)\n",
      "  Using cached torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from depyf==0.18.0->vllm) (0.3.9)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Requirement already satisfied: diskcache in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Using cached airportsdata-20250523-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (19.0.1)\n",
      "Collecting dill (from depyf==0.18.0->vllm)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets>=3.4.1->unsloth) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.4.1->unsloth)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.1)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.5 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub->unsloth)\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->vllm) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->vllm) (6.10.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.67.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.10->vllm) (2.27.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
      "INFO: pip is looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ray[cgraph]!=2.44.*,>=2.43.0 (from vllm)\n",
      "  Using cached ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached ray-2.45.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached ray-2.43.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\n",
      "INFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting compressed-tensors==0.9.4 (from vllm)\n",
      "  Using cached compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "INFO: pip is still looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.1->unsloth)\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.0-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
      "Collecting xgrammar==0.1.18 (from vllm)\n",
      "  Using cached xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools>=74.1.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (75.8.2)\n",
      "Collecting compressed-tensors==0.9.3 (from vllm)\n",
      "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchaudio==2.6.0 (from vllm)\n",
      "  Using cached torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.21.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (6.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.18.3)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.12/site-packages (from tyro->unsloth) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Using cached typeguard-4.4.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting typing_extensions>=4.10 (from vllm)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->vllm) (1.17.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.23.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->vllm) (0.2.1)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.4.1->unsloth) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Using cached unsloth-2025.6.2-py3-none-any.whl (276 kB)\n",
      "Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl (326.4 MB)\n",
      "Using cached compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
      "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "Using cached torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Using cached xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl (44.3 MB)\n",
      "Using cached xgrammar-0.1.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached ray-2.47.0-cp312-cp312-manylinux2014_x86_64.whl (68.9 MB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached gguf-0.17.0-py3-none-any.whl (95 kB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "Using cached lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Using cached mistral_common-1.6.2-py3-none-any.whl (6.5 MB)\n",
      "Using cached openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Using cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "Using cached opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached trl-0.18.2-py3-none-any.whl (366 kB)\n",
      "Using cached unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached tyro-0.9.24-py3-none-any.whl (128 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Using cached typeguard-4.4.3-py3-none-any.whl (34 kB)\n",
      "Using cached airportsdata-20250523-py3-none-any.whl (912 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl (105.3 MB)\n",
      "Using cached cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Installing collected packages: triton, py-cpuinfo, nvidia-cusparselt-cu12, fastrlock, blake3, typing_extensions, sympy, shtab, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numba, ninja, msgspec, llguidance, lark, jiter, interegular, hf-xet, hf_transfer, gguf, einops, docstring-parser, dill, cupy-cuda12x, astor, airportsdata, typeguard, tiktoken, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface_hub, depyf, tyro, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, diffusers, transformers, torch, prometheus-fastapi-instrumentator, opentelemetry-sdk, openai, lm-format-enforcer, datasets, xgrammar, xformers, torchvision, torchaudio, ray, outlines_core, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mistral_common, cut_cross_entropy, compressed-tensors, bitsandbytes, trl, peft, outlines, opentelemetry-exporter-otlp, unsloth_zoo, vllm, unsloth\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.61.0\n",
      "    Uninstalling numba-0.61.0:\n",
      "      Successfully uninstalled numba-0.61.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.31.0\n",
      "    Uninstalling opentelemetry-api-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.31.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface_hub 0.29.3\n",
      "    Uninstalling huggingface_hub-0.29.3:\n",
      "      Successfully uninstalled huggingface_hub-0.29.3\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.52b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.52b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.31.0\n",
      "    Uninstalling opentelemetry-sdk-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.31.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.2.1\n",
      "    Uninstalling datasets-2.2.1:\n",
      "      Successfully uninstalled datasets-2.2.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "  Attempting uninstall: ray\n",
      "    Found existing installation: ray 2.40.0\n",
      "    Uninstalling ray-2.40.0:\n",
      "      Successfully uninstalled ray-2.40.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires torch<2.6,>=2.2, but you have torch 2.6.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires torchvision<0.21.0,>=0.16.0, but you have torchvision 0.21.0 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires torch<2.6,>=2.2, but you have torch 2.6.0 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.0 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.3 which is incompatible.\n",
      "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed airportsdata-20250523 astor-0.8.1 bitsandbytes-0.46.0 blake3-1.0.5 compressed-tensors-0.9.3 cupy-cuda12x-13.4.1 cut_cross_entropy-25.1.1 datasets-3.6.0 depyf-0.18.0 diffusers-0.33.1 dill-0.3.8 docstring-parser-0.16 einops-0.8.1 fastrlock-0.8.3 gguf-0.17.0 hf-xet-1.1.3 hf_transfer-0.1.9 huggingface_hub-0.33.0 interegular-0.3.3 jiter-0.10.0 lark-1.2.2 llguidance-0.7.29 lm-format-enforcer-0.10.11 mistral_common-1.6.2 msgspec-0.19.0 multiprocess-0.70.16 ninja-1.11.1.4 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.86.0 opencv-python-headless-4.11.0.86 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 peft-0.15.2 prometheus-fastapi-instrumentator-7.1.0 protobuf-3.20.3 py-cpuinfo-9.0.0 pycountry-24.6.1 ray-2.47.0 shtab-1.7.2 sympy-1.13.1 tiktoken-0.9.0 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 transformers-4.52.4 triton-3.2.0 trl-0.18.2 typeguard-4.4.3 typing_extensions-4.14.0 tyro-0.9.24 unsloth-2025.6.2 unsloth_zoo-2025.6.1 vllm-0.8.5.post1 xformers-0.0.29.post2 xgrammar-0.1.18\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ff7813-1d46-4d93-af85-652637323ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.45.5\n",
      "  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting accelerate==1.6.0\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting xformers==0.0.29.post3\n",
      "  Using cached xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.12/site-packages (0.15.2)\n",
      "Collecting trl==0.15.2\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /opt/conda/lib/python3.12/site-packages (25.1.1)\n",
      "Requirement already satisfied: unsloth_zoo in /opt/conda/lib/python3.12/site-packages (2025.6.1)\n",
      "Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Using cached xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl (43.4 MB)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Installing collected packages: xformers, trl, bitsandbytes, accelerate\n",
      "  Attempting uninstall: xformers\n",
      "    Found existing installation: xformers 0.0.29.post2\n",
      "    Uninstalling xformers-0.0.29.post2:\n",
      "      Successfully uninstalled xformers-0.0.29.post2\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.18.2\n",
      "    Uninstalling trl-0.18.2:\n",
      "      Successfully uninstalled trl-0.18.2\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.46.0\n",
      "    Uninstalling bitsandbytes-0.46.0:\n",
      "      Successfully uninstalled bitsandbytes-0.46.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.2\n",
      "    Uninstalling accelerate-0.34.2:\n",
      "      Successfully uninstalled accelerate-0.34.2\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 trl-0.15.2 xformers-0.0.29.post3\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps bitsandbytes==0.45.5 accelerate==1.6.0 xformers==0.0.29.post3 peft trl==0.15.2 triton==3.2.0 cut_cross_entropy unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff4f163-1a67-431c-8b9d-d926490bac85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (3.20.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.12/site-packages (0.33.0)\n",
      "Requirement already satisfied: hf_transfer in /opt/conda/lib/python3.12/site-packages (0.1.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ef7756-3e27-4b30-bd78-f34febf5541a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring importlib_metadata: markers 'python_version < \"3.10\"' don't match your environment\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 1)) (5.5.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 7)) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 8)) (9.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (0.33.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 10)) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 11)) (3.20.3)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.115.11)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 13)) (3.9.5)\n",
      "Requirement already satisfied: openai>=1.52.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 14)) (1.86.0)\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 15)) (2.10.6)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 16)) (0.21.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 17)) (11.1.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 18)) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 19)) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 20)) (0.10.11)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 21)) (0.7.29)\n",
      "Requirement already satisfied: outlines==0.1.11 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 22)) (0.1.11)\n",
      "Requirement already satisfied: lark==1.2.2 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 23)) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.18 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 24)) (0.1.18)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 25)) (4.14.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 26)) (3.18.0)\n",
      "Requirement already satisfied: partial-json-parser in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 27)) (0.2.1.1.post5)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 28)) (26.3.0)\n",
      "Requirement already satisfied: msgspec in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 29)) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 30)) (0.17.0)\n",
      "Requirement already satisfied: mistral_common>=1.5.4 in /opt/conda/lib/python3.12/site-packages (from mistral_common[opencv]>=1.5.4->-r vllm_requirements.txt (line 32)) (1.6.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 33)) (4.11.0.86)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 34)) (6.0.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 35)) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from -r vllm_requirements.txt (line 36))\n",
      "  Using cached setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 37)) (0.8.1)\n",
      "Collecting compressed-tensors==0.9.4 (from -r vllm_requirements.txt (line 38))\n",
      "  Using cached compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: depyf==0.18.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 39)) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 40)) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 41)) (0.24.0)\n",
      "Requirement already satisfied: python-json-logger in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 42)) (2.0.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 43)) (1.15.2)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 44)) (1.11.1.4)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 45)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 47)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions-ai>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from -r vllm_requirements.txt (line 48)) (0.4.9)\n",
      "Requirement already satisfied: interegular in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.6.0)\n",
      "Requirement already satisfied: diskcache in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (5.6.3)\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (4.23.0)\n",
      "Requirement already satisfied: pycountry in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (20250523)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2.6.0)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.1.26)\n",
      "Requirement already satisfied: transformers>=4.38.0 in /opt/conda/lib/python3.12/site-packages (from xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (4.52.4)\n",
      "Requirement already satisfied: triton in /opt/conda/lib/python3.12/site-packages (from xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (3.2.0)\n",
      "Requirement already satisfied: astor in /opt/conda/lib/python3.12/site-packages (from depyf==0.18.0->-r vllm_requirements.txt (line 39)) (0.8.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from depyf==0.18.0->-r vllm_requirements.txt (line 39)) (0.3.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (2025.1.31)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (24.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.46.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (6.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.18.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9->-r vllm_requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9->-r vllm_requirements.txt (line 15)) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken>=0.6.0->-r vllm_requirements.txt (line 19)) (2024.11.6)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-sdk>=1.26.0->-r vllm_requirements.txt (line 45)) (0.47b0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->-r vllm_requirements.txt (line 46)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->-r vllm_requirements.txt (line 46)) (6.10.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.26.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.26.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp>=1.26.0->-r vllm_requirements.txt (line 47)) (1.26.0)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.5 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.0.7)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.34.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->-r vllm_requirements.txt (line 46)) (1.17.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.15.2)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.26.0->-r vllm_requirements.txt (line 46)) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.23.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.38.0->xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (0.5.3)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (8.1.8)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (15.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->-r vllm_requirements.txt (line 13)) (0.2.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /opt/conda/lib/python3.12/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.1.2)\n",
      "Using cached compressed_tensors-0.9.4-py3-none-any.whl (100 kB)\n",
      "Using cached setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: setuptools, compressed-tensors\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.8.2\n",
      "    Uninstalling setuptools-75.8.2:\n",
      "      Successfully uninstalled setuptools-75.8.2\n",
      "  Attempting uninstall: compressed-tensors\n",
      "    Found existing installation: compressed-tensors 0.9.3\n",
      "    Uninstalling compressed-tensors-0.9.3:\n",
      "      Successfully uninstalled compressed-tensors-0.9.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "conda 25.1.1 requires conda-libmamba-solver>=24.11.0, but you have conda-libmamba-solver 24.9.0 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.0 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\n",
      "vllm 0.8.5.post1 requires compressed-tensors==0.9.3, but you have compressed-tensors 0.9.4 which is incompatible.\n",
      "vllm 0.8.5.post1 requires xformers==0.0.29.post2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have xformers 0.0.29.post3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed compressed-tensors-0.9.4 setuptools-79.0.1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"vllm_requirements.txt\"):\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "\n",
    "!pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880956e3-6a33-4dcc-b2ac-55d32618f379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.6.0 in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /opt/conda/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (4.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (79.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.21.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.21.0) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.6.0) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f22228-aa75-4809-b1bb-ec71c1d2c441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==5.29.4\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.26.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.4 which is incompatible.\n",
      "unsloth-zoo 2025.6.1 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
      "vllm 0.8.5.post1 requires compressed-tensors==0.9.3, but you have compressed-tensors 0.9.4 which is incompatible.\n",
      "vllm 0.8.5.post1 requires xformers==0.0.29.post2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have xformers 0.0.29.post3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.29.4\n"
     ]
    }
   ],
   "source": [
    "# !pip show protobuf\n",
    "!pip install protobuf==5.29.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8140b22-1169-4758-9a47-205a0b3277fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 14:37:36.711142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 14:37:36.897371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749998256.925231    1293 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749998256.935664    1293 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-15 14:37:37.137468: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-15 14:37:44 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-15 14:37:44 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d2384-3eda-4f55-a052-2e6164ba07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "FastLanguageModel._fused_cross_entropy = False  # Disable buggy fused loss\n",
    "\n",
    "HF_TOKEN = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85eb872-554f-43c8-85d7-3a95003e5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8f718a-f72b-478f-b340-54fe8dea6906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 4096\n",
    "# max_seq_length = 5000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = HF_TOKEN, # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model.fused_linear_cross_entropy = None\n",
    "FastLanguageModel._fused_cross_entropy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a860f-193d-4471-9704-99f57485f1d6",
   "metadata": {},
   "source": [
    "## Data reading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d66a8e4-aaf6-4ab4-a814-d253be731e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid rows loaded: 1853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 1853\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "MAX_OCR_TOKENS = 4000  # Safe ceiling before model breaks\n",
    "\n",
    "def truncate_text(text, tokenizer, max_tokens):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n",
    "    return tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    safe_texts = []\n",
    "\n",
    "    for convo in convos:\n",
    "        # Truncate long OCR content if present\n",
    "        for turn in convo:\n",
    "            if 'content' in turn:\n",
    "                turn['content'] = truncate_text(turn['content'], tokenizer, MAX_OCR_TOKENS)\n",
    "        \n",
    "        try:\n",
    "            safe_texts.append(\n",
    "                tokenizer.apply_chat_template(\n",
    "                    convo,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Template error:\", e)\n",
    "            safe_texts.append(\"\")  # or optionally: continue\n",
    "    return {\"text\": safe_texts}\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()}\")\n",
    "                print(f\"Error: {e}\")\n",
    "    print(f\"Total valid rows loaded: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "file_path = \"filtered_conversations_new.jsonl\"\n",
    "data = read_jsonl(file_path)\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc06e156-b572-4673-9b54-ed352e657862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e7913a4b7d4f50be8afddbc0668eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=4):   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0309db183b4f1d9b64865bcc687cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c5d92ff3814fc5ae14edc64e197fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes — Train: 1575, Validation: 278\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Apply ShareGPT standardization\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "# Apply prompt formatting (already OCR-truncated in previous cell)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Remove any empty or broken examples after formatting\n",
    "dataset = dataset.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "# Shuffle + split\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Dataset sizes — Train: {len(train_dataset)}, Validation: {len(val_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa4a14-468c-478c-9472-1f1ce48e95a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff2a1d-d781-4bf8-98d1-cef376975745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7294e9cee3264dd9acaf21648df4ec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0937553e0b4f60af4998a01583fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7c42c393bc428695749afab9a13591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7069ec668a4a47ba12e336fa79687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth_zoo.dataset_utils import train_on_responses_only\n",
    "\n",
    "# STEP 1: Tokenize the datasets (required by train_on_responses_only)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=max_seq_length)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# STEP 2: Setup SFTTrainer using the tokenized datasets\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    dataset_text_field=None,  # Already tokenized; no text field needed\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        # warmup_steps = 5,\n",
    "        warmup_ratio = 0.15, \n",
    "        num_train_epochs = 4, # Set this for 1 full training run.\n",
    "        # max_steps = 200,\n",
    "        learning_rate = 8e-5,  \n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",  # Smoother than linear\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "        gradient_checkpointing = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "# STEP 3: Apply train_on_responses_only to isolate assistant targets\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd945d-f2fb-4654-9c54-58acb28b558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,575 | Num Epochs = 4 | Total steps = 788\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='788' max='788' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [788/788 4:59:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>0.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>0.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>0.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>0.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>0.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>0.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>0.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>0.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>0.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>0.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>0.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>0.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/llama-3.2-3b-instruct-bnb-4bit/resolve/main/config.json (Request ID: Root=1-684f07b7-43b78ffc1592956027944e44;89e59780-c57a-479f-b463-62124ccaf94b)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/llama-3.2-3b-instruct-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3.2-3b-instruct-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/llama-3.2-3b-instruct-bnb-4bit/resolve/main/config.json (Request ID: Root=1-684f2148-50bf7eaa49a24bea48dbebba;90a01a6d-8c37-411d-8a0d-b2991946dda6)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/llama-3.2-3b-instruct-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3.2-3b-instruct-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Duration: 299.61 minutes\n",
      "Final training loss: 0.08069543181088959\n"
     ]
    }
   ],
   "source": [
    "# # trainer_stats = trainer.train()\n",
    "# import time\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "trainer_stats = trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Duration: {(end_time - start_time)/60:.2f} minutes\")\n",
    "print(f\"Final training loss: {trainer_stats.metrics.get('train_loss')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf951f4f-3a6e-4b39-9963-1dac8017bd2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Validation loss eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d415b07-62c3-4155-a03b-497963af1c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f976c45c2a34c6b879c0e737cd297ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 278/278 [03:47<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Eval Loss: 1.5012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def truncate_for_eval(batch):\n",
    "    tokens = tokenizer(batch[\"text\"], truncation=True, max_length=4096)[\"input_ids\"]\n",
    "    return {\"text\": tokenizer.decode(tokens, skip_special_tokens=True)}\n",
    "\n",
    "val_dataset = val_dataset.map(truncate_for_eval, num_proc=2)\n",
    "\n",
    "model.eval()\n",
    "total_loss, total_samples = 0, 0\n",
    "\n",
    "for batch in tqdm.tqdm(val_dataset, desc=\"Evaluating\"):\n",
    "    inputs = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096  # Explicit truncation\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        total_loss += outputs.loss.item()\n",
    "        total_samples += 1\n",
    "\n",
    "print(f\"Mean Eval Loss: {total_loss / total_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c710e1e-9194-4fca-8ff8-26dcefc62bd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eced3de1-df56-4da9-801d-52f8047d8184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/llama-3.2-3b-instruct-bnb-4bit/resolve/main/config.json (Request ID: Root=1-684f22c4-2f1012c228951c4679caa561;baae3fbd-90fd-446b-b4d5-d078abdd1414)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/llama-3.2-3b-instruct-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/llama-3.2-3b-instruct-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('test8_doc-splitter-llama-3-2-3B-20-epoch/tokenizer_config.json',\n",
       " 'test8_doc-splitter-llama-3-2-3B-20-epoch/special_tokens_map.json',\n",
       " 'test8_doc-splitter-llama-3-2-3B-20-epoch/chat_template.jinja',\n",
       " 'test8_doc-splitter-llama-3-2-3B-20-epoch/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"test8_doc-splitter-llama-3-2-3B-20-epoch\")  # Local saving\n",
    "tokenizer.save_pretrained(\"test8_doc-splitter-llama-3-2-3B-20-epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc10e49-e15e-4227-af3a-679b3cb9deb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaModel' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## pushing whole merged model to hf (16-bit)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m HF_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_vQYmYeROYXyTtTMPiHCAIkwLcAdvnRjEDb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_merged\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeerakwyne/test8_doc-splitter-llama-3-2-3B-20-epoch_merged\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#  avoids the crash\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2424\u001b[0m, in \u001b[0;36munsloth_generic_push_to_hub_merged\u001b[0;34m(self, repo_id, tokenizer, save_method, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2424\u001b[0m \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2426\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2324\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m save_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_4bit_forced\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2322\u001b[0m     save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2324\u001b[0m \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:655\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, save_method, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    646\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m files found for the base model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(max_size_in_bytes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m total_size_in_bytes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    649\u001b[0m (\n\u001b[1;32m    650\u001b[0m     username, repo_id, hf_api, token,\n\u001b[1;32m    651\u001b[0m     output_dtype, element_size,\n\u001b[1;32m    652\u001b[0m     lora_weights, state_dict, save_size, free,\n\u001b[1;32m    653\u001b[0m     temp_file, save_directory, new_use_temp_file,\n\u001b[1;32m    654\u001b[0m     low_disk_space_usage, max_shard_size_in_bytes,\n\u001b[0;32m--> 655\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_saving\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5GB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_into_original\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_size_in_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_size_in_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m use_temp_file \u001b[38;5;241m=\u001b[39m use_temp_file \u001b[38;5;129;01mor\u001b[39;00m new_use_temp_file\n\u001b[1;32m    669\u001b[0m _save_dir_path \u001b[38;5;241m=\u001b[39m Path(save_directory)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:462\u001b[0m, in \u001b[0;36mprepare_saving\u001b[0;34m(model, save_directory, push_to_hub, max_shard_size, private, token, output_dtype, merge_into_original, low_disk_space_usage, min_size_in_bytes, use_temp_file)\u001b[0m\n\u001b[1;32m    459\u001b[0m element_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype \u001b[38;5;241m=\u001b[39m output_dtype)\u001b[38;5;241m.\u001b[39melement_size()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Get state_dict\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m lora_weights, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_lora_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_into_original\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmerge_into_original\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_state_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Total save size in bytes\u001b[39;00m\n\u001b[1;32m    468\u001b[0m save_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(get_torch_storage_size_new(x, element_size) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:344\u001b[0m, in \u001b[0;36mcreate_lora_statistics\u001b[0;34m(model, merge_into_original, return_state_dict)\u001b[0m\n\u001b[1;32m    341\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_state_dict: \u001b[43massert_same_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lora_weights, state_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:226\u001b[0m, in \u001b[0;36massert_same_keys\u001b[0;34m(model, new_state_dict)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massert_same_keys\u001b[39m(model, new_state_dict):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# All Unsloth Zoo code licensed under LGPLv3\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     inner_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m model\n\u001b[1;32m    227\u001b[0m     original_keys \u001b[38;5;241m=\u001b[39m inner_model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    228\u001b[0m     all_original_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaModel' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "## pushing whole merged model to hf (16-bit)\n",
    "\n",
    "HF_TOKEN = \" \"\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_id= \" \",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token= HF_TOKEN,\n",
    "    use_temp_dir=False  #  avoids the crash\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab848856-7a8b-44aa-af27-f58e5bcbebd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeerakwyne/test8_doc-splitter-llama-3-2-3B-20-epoch_Lora\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlora\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2424\u001b[0m, in \u001b[0;36munsloth_generic_push_to_hub_merged\u001b[0;34m(self, repo_id, tokenizer, save_method, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2424\u001b[0m \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2426\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2324\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m save_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_4bit_forced\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2322\u001b[0m     save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2324\u001b[0m \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:655\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, save_method, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    646\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m files found for the base model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(max_size_in_bytes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m total_size_in_bytes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    649\u001b[0m (\n\u001b[1;32m    650\u001b[0m     username, repo_id, hf_api, token,\n\u001b[1;32m    651\u001b[0m     output_dtype, element_size,\n\u001b[1;32m    652\u001b[0m     lora_weights, state_dict, save_size, free,\n\u001b[1;32m    653\u001b[0m     temp_file, save_directory, new_use_temp_file,\n\u001b[1;32m    654\u001b[0m     low_disk_space_usage, max_shard_size_in_bytes,\n\u001b[0;32m--> 655\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_saving\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5GB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_into_original\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_size_in_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_size_in_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m use_temp_file \u001b[38;5;241m=\u001b[39m use_temp_file \u001b[38;5;129;01mor\u001b[39;00m new_use_temp_file\n\u001b[1;32m    669\u001b[0m _save_dir_path \u001b[38;5;241m=\u001b[39m Path(save_directory)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:510\u001b[0m, in \u001b[0;36mprepare_saving\u001b[0;34m(model, save_directory, push_to_hub, max_shard_size, private, token, output_dtype, merge_into_original, low_disk_space_usage, min_size_in_bytes, use_temp_file)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Too small - try using the temporary file system (sometimes large like Kaggle)\u001b[39;00m\n\u001b[1;32m    509\u001b[0m try_temp_file \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory(ignore_cleanup_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 510\u001b[0m try_save_directory \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    512\u001b[0m total, used, free \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mdisk_usage(save_directory)\n\u001b[1;32m    513\u001b[0m free \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(free\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\" \", tokenizer, save_method=\"lora\", token= HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f948f-3df8-4448-a8d2-cbba79847ac2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c5197-b2f9-4cb3-afa8-2df6248a0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "### Loading Lora saved moel \n",
    "\n",
    "#### adding trained lora to base model (just replace the adapter part)\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import get_chat_template\n",
    "\n",
    "\n",
    "HF_TOKEN = \" \"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = True,\n",
    "    token = HF_TOKEN,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Now load adapter (manually) this is basically only the parameter we trained\n",
    "model.load_adapter(\"test8_doc-splitter-llama-3-2-3B-20-epoch\")\n",
    "\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0177ee-fdff-47aa-930e-cfd941e7da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffa2b73859a42ec8ee26f2315fd2aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "### Loading merged_16-bit model\n",
    "\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from unsloth import get_chat_template\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "repo_id = \" \"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_TOKEN\"] = HF_TOKEN  # also respected by transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, token=HF_TOKEN)\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "result = FastLanguageModel.from_pretrained(\n",
    "    model_name = repo_id,\n",
    "    max_seq_length = 4096,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = False,\n",
    "    device_map = \"auto\",\n",
    "    token = HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Handle both cases\n",
    "if isinstance(result, tuple):\n",
    "    model, _ = result\n",
    "else:\n",
    "    model = result\n",
    "\n",
    "# Now check model type\n",
    "print(\"Model type:\", type(model))\n",
    "\n",
    "# Proceed to inference prep\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972a546-dfb0-469b-96f1-d60186aad409",
   "metadata": {},
   "source": [
    "### Inference + testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e53b81-5576-41fd-aac6-e243b4f873e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "#Inference function\n",
    "def predict_document_type_debug(conversations, max_tokens=128, temperature=0.8, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Return both cleaned and raw model outputs for inspection.\n",
    "    \"\"\"\n",
    "    if not conversations or not isinstance(conversations, list):\n",
    "        print(\"[DEBUG] No conversation content found.\")\n",
    "        return \"no_input\", None\n",
    "\n",
    "    user_prompt = conversations[0].get(\"content\", \"\")\n",
    "    if not user_prompt.strip():\n",
    "        print(\"[DEBUG] User prompt is empty.\")\n",
    "        return \"no_input\", None\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode raw output with special tokens\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract response\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in decoded_output:\n",
    "        response = decoded_output.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "    else:\n",
    "        response = decoded_output\n",
    "\n",
    "    # Clean common special tokens\n",
    "    cleaned = (\n",
    "        response.replace(\"<|eot_id|>\", \"\")\n",
    "                .replace(\"<|end_of_turn|>\", \"\")\n",
    "                .strip()\n",
    "    )\n",
    "\n",
    "    return cleaned, decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e19be80b-b8e1-4c00-b60d-e77d4ac8a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned, raw  = predict_document_type_debug(val_dataset[120][\"conversations\"])\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d67f5439-b79c-4bfe-a599-e74389381050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an expert in identifying document types using OCR text. Your role is to look at the OCR texts inputs provided by the user and classify them into different document types. \\nYou will return the response in Output format defined below.\\n\\nThe possible document types are: [\\'mill_certificate\\', \\'invoice\\', \\'bill_of_lading\\', \\'packing_list\\'].\\n\\nExample:\\nInput: \\'{\"ocr\": [\"page 1 text\", \"page 2 text\"], \"num_pages\": 2}\\'\\nOutput: \\'{\"mill_certificate\": [[0], [1]]}\\'\\n\\nInput: {\"ocr\": [\"INSPECTION CERTIFICATE\\\\nContract No.\\\\nShipper\\\\nOrder No.\\\\nCustomer\\\\nCommodity\\\\nSpecification\\\\nDelivery\\\\nCondition\\\\n7XY3B609 Sheet No.\\\\nF721\\\\nSAPPORO METALS CORP.\\\\nHOKKAIDO WORKS\\\\nSapporo Rod & Bar Division\\\\n4-12 Minami, Chuo-ku, Sapporo, Japan\\\\nDate of Issue 2023-11-21 P.01/01-02\\\\nReference No. BST990PT\\\\nKE23A09-T0017-1\\\\nNIPPON STEEL TRADING CO., LTD.\\\\nOCH-2305Q\\\\nIP\\\\nSection Code\\\\n157 BJK8\\\\nCustomer No.\\\\nGLOBAL MANUFACTURING INC.\\\\nCOLD DRAWN STEEL BARS\\\\nKNY8P-D\\\\nANNEALED\\\\nSize\\\\nMM\\\\n12.0 S\\\\nSpec.\\\\nItem\\\\nUnit\\\\nMin.\\\\nMax.\\\\nHeat No.\\\\nProduct Details\\\\nNUMBER OF BARS\\\\nResults\\\\n3C7011\\\\nMASS\\\\nChemical\\\\nC\\\\nSI\\\\nMN\\\\nP\\\\nS\\\\nCU\\\\nNI\\\\nCR\\\\nAL\\\\nV\\\\n0\\\\nΤΙ\\\\nACTUAL\\\\nKG\\\\n7\\\\n14,563\\\\nComposition\\\\nX100\\\\nX100\\\\nX100\\\\nLOML\\\\nTOO\\\\n70\\\\n64\\\\n210\\\\n188\\\\n60\\\\n90\\\\n75\\\\nX1000\\\\n25\\\\nX1000\\\\n25\\\\nX100\\\\n20\\\\nX100\\\\n25\\\\n60\\\\nX100\\\\n100\\\\n120\\\\n95\\\\nX1000\\\\n4\\\\n3\\\\nX1000\\\\n60\\\\n120\\\\n80\\\\nX10000\\\\n15\\\\nX10000\\\\n35\\\\nTR.\\\\nNon Metallic Inclusions Test\\\\nJIS\\\\nDA\\\\nDB\\\\nDC\\\\nD(A+B+C)\\\\nMAX-T\\\\nSURFACE AREA (NO.\\\\nSURFACE AREA\\\\n(NO.2)\\\\nSURFACE AREA\\\\n(NO.3)\\\\nSURFACE AREA (NO.4)\\\\n0000000000 Σ\\\\n0.00\\\\n0.00\\\\n0.00\\\\n0.00\\\\n0.00\\\\n0.00\\\\n0.05\\\\n0.00\\\\n0.00\\\\nMICRON\\\\nMICRON\\\\nMICRON\\\\nMICRON\\\\n5-10 MICRON\\\\n20\\\\n****\\\\n* LESS THAN 5 MICRON\\\\nCLASSIFICATION BY\\\\nMAX-T LEVEL\\\\nDecarburization\\\\nDM-T MIN.\\\\nMAX.\\\\nVisual & Dimensional\\\\nSurface Quality\\\\nCASTER NO.\\\\n11-15\\\\n16-\\\\nMICRON\\\\nMICRON\\\\nTest\\\\nMM\\\\n0.05\\\\n0.05\\\\n0.05\\\\nInspection\\\\nGOOD\\\\nGOOD\\\\nSurveyor\\\\nWe hereby certify that the material herein\\\\ndescribed has been manufactured in accordance\\\\nwith the standards and specification specified\\\\nby you that it satisfies the requirement.\\\\nHiroshi Nakamura\\\\nHIROSHI NAKAMURA\\\\nManager of Sapporo Quality section\\\\n\\\\n---\\\\n\\\\n青钢集团 青岛钢铁有限公司\\\\nQINGDAO IRON & STEEL CO., LTD.\\\\n产品质量证明书\\\\nINSPECTION CERTIFICATE\\\\nACCORDING TO EN 10204 3.1\\\\n收货单位\\\\nCustomer\\\\nZHANGJIANG SUPPLY CHAIN CO., LTD.\\\\n产品名称\\\\nProduct name\\\\n牌号\\\\nSteel Grade.\\\\n规格\\\\nSize(mm)\\\\n交货标准\\\\n|冷轧钢板 COLD ROLLED STEEL SHEET\\\\n| QG235C\\\\n[T2.5×1500\\\\nGB/T3274-2017、EN10130-2006\\\\nSpecification\\\\n交货状态\\\\nDelivery\\\\n+CR\\\\nCondition\\\\n日期\\\\nDate\\\\n2024-06-25\\\\n控制号\\\\nControl Number.\\\\n山东省青岛市经济技术开发区\\\\n266500\\\\n邮编\\\\nNo. 5 Economic Development Zone, Qingdao\\\\nCity,\\\\nShandong Province, P.R.China\\\\nPostcode:266500,\\\\n1223\\\\nTel:400-790-\\\\n证明书号\\\\n| QD20240625A4572900330001\\\\nCertificate No.\\\\n车号\\\\nTrain No.\\\\n合同编号\\\\nContract No.\\\\n签发日期\\\\n| 鲁B05678G\\\\nQDAA473016/24QDE-C-7-198\\\\n2024-06-25\\\\nDate of delivery\\\\n轧制批号\\\\n炉号\\\\n长度\\\\n支数\\\\nRolling Batch No.\\\\nHEAT No.\\\\nLength(mm)\\\\nPieces\\\\n224060789\\\\nB24902190\\\\n6000.000\\\\n25\\\\n合计 Total\\\\n25\\\\n化学成分(%) chemical composition\\\\n重量\\\\nWeight (t)\\\\n9.432\\\\n9.432\\\\nCl0.21\\\\n熔炼\\\\nSmelting\\\\nNbj0.00.5\\\\nSi 0.30\\\\nMo 0.02\\\\nMn | 1.15\\\\nSn|0.00\\\\nP|0.028\\\\nCE|0.40\\\\nS 0.018\\\\nCu|0.02\\\\nNi 0.01\\\\nCr 0.05\\\\nV/0.020\\\\n二、力学性能 mechanical properties\\\\n224060789\\\\n常规拉伸Conventional stretching\\\\n屈服强度R(MPa) yield strength\\\\n抗拉强度Rm(MPa) tensile strength\\\\n屈强比 yield to tensile ratio\\\\n(标距Gauge length: 200mm)\\\\n412\\\\n568\\\\n0.73\\\\n26.0\\\\n备注\\\\nREMARKS\\\\n伸长率(%) percentage elongation\\\\n[CE=C+Mn/6+(Cr+Mo+V)/5+(Ni+Cu)/15\\\\n|此质证书所示批号外形尺寸和表面质量合格。\\\\nSurface quality and Dimension of the Rolling Batch Number in this Certificate are qualified.\\\\n注释\\\\nNOTES\\\\n|说明:\\\\n1.复印件不具备同等效力;\\\\n会验者\\\\n12.钢牌号以质量证明书为准。\\\\nSURVEYOR TO\\\\nExplains:\\\\n1. The duplicate should not be equally authentic as the original inspection\\\\ncertificate\\'s.\\\\n|2.The steel grade is subject to the inspection certificate\\'s.\\\\n质量负责\\\\nQUALITY MANAGE\\\\n质量专用掌\\\\n(ZJ-QDS).\", \"三菱製鋼株式会社  \\\\nMITSUBISHI STEEL CORPORATION  \\\\n〒105-0001 東京都港区虎ノ門4丁目1-1  \\\\n4-1-1, TORANOMON, MINATO-KU, TOKYO, 105-0001, JAPAN  \\\\n〒671-1687 兵庫県姫路市大津区1丁目5-2  \\\\n1-5-2, OTSU, HIMEJI-CITY, HYOGO, 671-1687, JAPAN  \\\\n\\\\n鋼材検査証明書  \\\\nINSPECTION CERTIFICATE  \\\\n\\\\n需要家  \\\\nCUSTOMER  \\\\nHYUNDAI STEEL COMPANY  \\\\n需要家管理番号  \\\\nCUSTOMER\\'S CONTROL No.  \\\\nGSC-00123  \\\\n\\\\n社  \\\\nHEAD OFFICE  \\\\n姫路製鉄所  \\\\nHIMEJI WORKS  \\\\n\\\\n証明書番号  \\\\nCERTIFICATE No.  \\\\n21H4005673  \\\\n\\\\n発行日  \\\\nDATE OF ISSUE  \\\\n2023-08-30  \\\\n\\\\n注文者  \\\\nORDERER  \\\\nSUMITOMO CORPORATION  \\\\n\\\\n注文者照合番号  \\\\nREFERENCE No.  \\\\nSCJ-2023-XY  \\\\n\\\\n契約番号  \\\\nCONTRACT No.  \\\\n3-305-M5-9-Y-750J  \\\\n\\\\n品名  \\\\nCOMMODITY  \\\\nCOLD ROLLED STEEL IN COIL  \\\\n\\\\n規格  \\\\nSPECIFICATION  \\\\nMITSUBISHI STEEL STD CR01-B MATTE FINISH  \\\\n\\\\n品 目  \\\\nITEM No.  \\\\nSIZE  \\\\n1000MM X 500MM  \\\\n\\\\n管理番号  \\\\nCONTROL No.  \\\\nCOIL No.  \\\\n製鋼番号  \\\\nHEAT No.  \\\\n\\\\n数量  \\\\nQUANTITY  \\\\n質量  \\\\nNET MASS  \\\\n化学成分  \\\\nCHEMICAL COMPOSITION  \\\\n\\\\n(%)  \\\\nC  \\\\nSi  \\\\nMn  \\\\nP  \\\\nS  \\\\nAl  \\\\n\\\\n1  \\\\n101  \\\\nCR-23012  \\\\n(HJ3489)  \\\\n2  \\\\n18000  \\\\n56  \\\\n0.03  \\\\n0.25  \\\\n0.35  \\\\n0.012  \\\\n0.005  \\\\n0.032  \\\\n\\\\n2  \\\\n102  \\\\nCR-23013  \\\\n(HJ3490)  \\\\n2  \\\\n17500  \\\\n55  \\\\n0.04  \\\\n0.26  \\\\n0.36  \\\\n0.014  \\\\n0.006  \\\\n0.031  \\\\n\\\\n** 合計 **  \\\\nTOTAL  \\\\n4  \\\\n35500  \\\\n\\\\n注釈  \\\\nNOTES  \\\\nCR: COLD ROLLED  \\\\nHARDNESS: HV250  \\\\nMADE IN JAPAN +81-79-239-1234  \\\\n\\\\n<NOTICE>  \\\\n上記注文品は御指定の規格または仕様に従って製造され、その要求事項を満足していることを証明します。  \\\\nWE HEREBY CERTIFY THAT THE MATERIAL DESCRIBED HEREIN HAS BEEN MADE IN ACCORDANCE WITH THE RULES OF THE CONTRACT.  \\\\n\\\\nお問い合わせ先:  \\\\nmsq-himeji@jp.mitsubishisteel.com  \\\\n\\\\n山本 直樹  \\\\nNAOKI YAMAMOTO  \\\\n姫路製鉄所 品質保証室長  \\\\nHead of Quality Assurance Dept.  \\\\nHIMEJI WORKS  \\\\n\\\\nこのミルシートは特殊偽造防止技術を使用しております  \\\\nThis Inspection Certificate is printed with special forgery proof technology.\"], \"num_pages\": 2}\\n\\nOutput format: \\'{\"document_type\": [[page_numbers]]}\\'\\n\\nYour response should only contain the string in the provided output format and no other text.\\n',\n",
       "  'role': 'user'},\n",
       " {'content': '{\"mill_certificate\": [[0], [1]]}', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[120][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d558778a-4754-4448-a1e6-9224ec20af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"packing_list\": [[0]]}\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '[USER]: You are an expert in identifying document types using OCR text. Your role is to look at the OCR texts inputs provided by the user and classify them into different document types. \\\n",
    "You will return the response in Output format defined below.\\n\\n\\\n",
    "The possible document types are: [\\'mill_certificate\\', \\'invoice\\', \\'bill_of_lading\\', \\'packing_list\\'].\\n\\n\\\n",
    "Example:\\n\\\n",
    "Input: \\'{\"ocr\": [\"page 1 text\", \"page 2 text\"], \"num_pages\": 2}\\'\\n\\\n",
    "Output: \\'{\"bill_of_lading\": [[0], [1]]}\\'\\n\\n\\\n",
    "Input: {\"ocr\": [\"Packing List\\nExporter: Global Trade Ltd\\nConsignee: Star Imports Inc\\nContents:\\n- 100 units Widget A\\n- 50 units Widget B\\nTotal weight: 550kg\"], \"num_pages\": 1}\\n\\n\\\n",
    "Output format: \\'{\"document_type\": [[page_numbers]]}\\'\\n\\n\\\n",
    "Your response should only contain the string in the provided output format and no other text.\\n\\n\\\n",
    "[ASSISTANT]:'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "### answer should be packing_list: [[0]]\n",
    "\n",
    "cleaned, raw  = predict_document_type_debug(sample[\"conversations\"])\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a5e5b87-145e-499b-a136-01d187e9a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"bill_of_lading\": [[0]], \"mill_certificate\": [[1]]}\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '''[USER]: You are an expert in identifying document types using OCR text. Your role is to look at the OCR texts inputs provided by the user and classify them into different document types. \n",
    "You will return the response in Output format defined below.\n",
    "\n",
    "The possible document types are: ['mill_certificate', 'invoice', 'bill_of_lading', 'packing_list'].\n",
    "\n",
    "Example:\n",
    "Input: '{\"ocr\": [\"page 1 text\", \"page 2 text\"], \"num_pages\": 2}'\n",
    "Output: '{\"bill_of_lading\": [[0], [1]]}'\n",
    "\n",
    "Input: {\"ocr\": [\n",
    "\"Invoice\\nINVOICE #4567\\nDate: 2024-11-12\\nSeller: Alpha Electronics Ltd.\\nBuyer: Tech World Co.\\nItems:\\n- 10x SSD 1TB @ $100\\n- 5x Monitor 24\\\" @ $150\\nTotal Amount Due: $1,750\\nPayment Terms: Net 30 Days\",\n",
    "\"Mill Certificate\\nCertificate No: 9982\\nManufacturer: SteelCorp Industries\\nProduct: Cold Rolled Steel Sheets\\nSpecification: ASTM A1008\\nHeat No: 558930\\nMechanical Properties:\\n- Yield Strength: 280 MPa\\n- Tensile Strength: 420 MPa\\nCertified by: QA Engineer - John Smith\"\n",
    "], \"num_pages\": 2}\n",
    "\n",
    "Output format: '{\"document_type\": [[page_numbers]]}'\n",
    "\n",
    "Your response should only contain the string in the provided output format and no other text.\n",
    "\n",
    "[ASSISTANT]:'''\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "###output should be {\"invoice\": [[0]], \"mill_certificate\": [[1]]}\n",
    "\n",
    "cleaned, raw  = predict_document_type_debug(sample[\"conversations\"])\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ba66c66-29be-4b85-a430-95c686ffd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluator import DocumentEvaluator, evaluate_strict_document_groups\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_valid_json(text):\n",
    "    try:\n",
    "        json.loads(text)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run_combined_evaluation(dataset, predict_fn, log_top_k_slowest=3, iou_threshold=1.0):\n",
    "    evaluator = DocumentEvaluator()\n",
    "    raw_preds = []\n",
    "    raw_trues = []\n",
    "    invalid_logs = []\n",
    "    inference_times = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n",
    "        sample = dataset[i]\n",
    "\n",
    "        # Extract true response\n",
    "        true_text = \"\"\n",
    "        for turn in sample.get('conversations', []):\n",
    "            if turn.get('role') == 'assistant':\n",
    "                true_text = turn.get('content', \"\")\n",
    "                break\n",
    "\n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        pred_text = predict_fn(sample)\n",
    "        elapsed = time.time() - start_time\n",
    "        inference_times.append((i, elapsed))\n",
    "\n",
    "        if not is_valid_json(pred_text):\n",
    "            invalid_logs.append({\n",
    "                \"index\": i,\n",
    "                \"input\": sample.get(\"conversations\", [])[0][\"content\"],\n",
    "                \"expected\": true_text,\n",
    "                \"prediction\": pred_text\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        raw_preds.append(pred_text)\n",
    "        raw_trues.append(true_text)\n",
    "        evaluator.add_sample(pred_text, true_text)\n",
    "\n",
    "    # Save invalid predictions\n",
    "    with open(\"invalid_predictions_log.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in invalid_logs:\n",
    "            f.write(f\"--- Sample {item['index']} ---\\n\")\n",
    "            f.write(\"Input:\\n\" + item[\"input\"] + \"\\n\")\n",
    "            f.write(\"Expected:\\n\" + item[\"expected\"] + \"\\n\")\n",
    "            f.write(\"Prediction:\\n\" + item[\"prediction\"] + \"\\n\\n\")\n",
    "\n",
    "    # Run evaluations\n",
    "    print(\"\\n===== LEVEL 1: Document Classification =====\")\n",
    "    level1_metrics = evaluator.evaluate()\n",
    "\n",
    "    print(\"===== LEVEL 2 & 3: Group-Level Evaluation =====\")\n",
    "    level23_metrics = evaluate_strict_document_groups(raw_preds, raw_trues, iou_threshold=iou_threshold)\n",
    "\n",
    "    total_time = sum(t for _, t in inference_times)\n",
    "    valid_samples = len(raw_preds)\n",
    "    all_samples = len(dataset)\n",
    "\n",
    "    print(\"\\n===== Inference Timing =====\")\n",
    "    print(f\"Total time: {total_time:.2f} sec\")\n",
    "    print(f\"Avg time per valid sample: {total_time / max(valid_samples, 1):.2f} sec\")\n",
    "    print(f\"Avg time per all samples: {total_time / all_samples:.2f} sec\")\n",
    "\n",
    "    print(f\"\\n===== Top {log_top_k_slowest} Slowest Samples =====\")\n",
    "    inference_times.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, t in inference_times[:log_top_k_slowest]:\n",
    "        print(f\"Sample {i}: {t:.2f} sec\")\n",
    "\n",
    "    return {\n",
    "        \"document_level\": level1_metrics,\n",
    "        \"segmentation_level\": level23_metrics,\n",
    "        \"timing\": {\n",
    "            \"total_seconds\": total_time,\n",
    "            \"avg_per_valid_sample\": total_time / max(valid_samples, 1),\n",
    "            \"avg_per_all_sample\": total_time / all_samples,\n",
    "            \"slowest_samples\": inference_times[:log_top_k_slowest]\n",
    "        },\n",
    "        \"invalid_samples\": invalid_logs,\n",
    "    }\n",
    "\n",
    "def wrapped_predict_fn(sample):\n",
    "    return predict_document_type_debug(sample[\"conversations\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db6d00c-b50e-4efe-be45-a5baed170e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 178/178 [03:33<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LEVEL 1: Document Classification =====\n",
      "\n",
      "===== DOCUMENT CLASSIFICATION EVALUATION RESULTS =====\n",
      "\n",
      "----- Document-Level Metrics -----\n",
      "Number of documents: 176\n",
      "Exact match rate: 0.8750\n",
      "Partial match rate: 0.1193\n",
      "Macro Precision: 0.7837\n",
      "Macro Recall: 0.7405\n",
      "Macro F1: 0.7615\n",
      "\n",
      "----- Class-Level Metrics -----\n",
      "Class                 Precision     Recall         F1    Support\n",
      "-----------------------------------------------------------------\n",
      "packing_list             0.7286     0.5667     0.6375         90\n",
      "mill_certificate         0.6494     0.5882     0.6173         85\n",
      "bill_of_lading           0.9140     0.8854     0.8995         96\n",
      "invoice                  0.8429     0.9219     0.8806        192\n",
      "\n",
      "----- Macro Averages -----\n",
      "Macro Precision: 0.7837\n",
      "Macro Recall: 0.7405\n",
      "Macro F1: 0.7615\n",
      "\n",
      "===== END OF EVALUATION RESULTS =====\n",
      "\n",
      "===== LEVEL 2 & 3: Group-Level Evaluation =====\n",
      "\n",
      "===== Inference Timing =====\n",
      "Total time: 212.84 sec\n",
      "Avg time per valid sample: 1.21 sec\n",
      "Avg time per all samples: 1.20 sec\n",
      "\n",
      "===== Top 3 Slowest Samples =====\n",
      "Sample 120: 7.43 sec\n",
      "Sample 135: 7.42 sec\n",
      "Sample 48: 6.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document_level': {'exact_match_rate': 0.875,\n",
       "  'partial_match_rate': 0.11931818181818182,\n",
       "  'no_match_rate': 0.005681818181818232,\n",
       "  'average_overlap': 0.7836894288507191,\n",
       "  'macro_precision': 0.7836894288507191,\n",
       "  'macro_recall': 0.740548406862745,\n",
       "  'macro_f1': 0.7615084003460919,\n",
       "  'num_samples': 176},\n",
       " 'segmentation_level': {'level2_group_exact_match_rate': 0.9034090909090909,\n",
       "  'level3_segmentation_recall': 0.9597523219814241,\n",
       "  'over_segmentation_rate': 0.3693181818181818,\n",
       "  'under_segmentation_rate': 0.4090909090909091},\n",
       " 'timing': {'total_seconds': 212.84353160858154,\n",
       "  'avg_per_valid_sample': 1.2093382477760315,\n",
       "  'avg_per_all_sample': 1.1957501775763009,\n",
       "  'slowest_samples': [(120, 7.426151752471924),\n",
       "   (135, 7.42256236076355),\n",
       "   (48, 6.080994367599487)]},\n",
       " 'invalid_samples': [{'index': 120,\n",
       "   'input': 'You are an expert in identifying document types using OCR text. Your role is to look at the OCR texts inputs provided by the user and classify them into different document types. \\nYou will return the response in Output format defined below.\\n\\nThe possible document types are: [\\'mill_certificate\\', \\'invoice\\', \\'bill_of_lading\\', \\'packing_list\\'].\\n\\nExample:\\nInput: \\'{\\'ocr\\': [\"page 1 text\", \"page 2 text\"], \\'num_pages\\': 2}\\'\\nOutput: \\'{\"others\": [[1]], \"invoice\": [[0]]}\\'\\n\\nInput: {\\'ocr\\' : [\"Importador (Importer):\\\\nHISENSE MONTERREY HOME APPLIANCE\\\\nMANUFACTURING S. DE R.L. DE C.V.\\\\nAVENIDA PUERTO GRANDE No. 1301-B OTRA NO\\\\nESPECIFICADA EN EL CATALOGO SALINAS VICTORIA, NL\\\\n65500 MEX\\\\nIMMEX: 201-2021, RFC: HMH200908BX7\\\\nTel: Fax:\\\\nRemitente (Sender): HISENSE (HONG KONG) AMERICA MANUFACTURING CO. LIMITED\\\\nCAMINO OESTE A CONNAUGHT, No. 148 CENTRO COMERCIAL SINGGA, OFICINA 3101-05 HONG\\\\nKONG (REGION ADMINISTRATIVA ESPECIAL DE LA RE, 999077 CHN\\\\nIMMEX:, RFC: 71731373\\\\nTel: Fax:\\\\nVendido a (Sold To):\\\\nHISENSE MONTERREY HOME APPLIANCE MANUFACTURING S. DE R.L. DE\\\\nC.V.\\\\nAVENIDA PUERTO GRANDE No. 1301-B OTRA NO ESPECIFICADA EN EL CATALOGO\\\\nSALINAS VICTORIA, NL 65500 MEX\\\\nIMMEX: 201-2021, RFC: HMH200908BX7\\\\nTel: Fax:\\\\nINCREMENTABLES\\\\nFletes (Freight): 0.00\\\\nSeguros (Insurance): 0\\\\nEmbalajes: 0\\\\nOtros (Others): 0\\\\nTotal: 0\\\\nFACTURA IMPORTACION BILINGÜE (Clave: IN)\\\\n(BILINGUAL IMPORT INVOICE (Code:IN))\\\\nFactura (Invoice No.): 1598435H-4\\\\nFecha (Date)(mm/dd/yyyy): 10/20/2024\\\\nT. Cambio (Exch. Rate): 1\\\\nTipo de Envio (Ship Type): NOAPPLY\\\\nCOVE (eDocument):\\\\nT.Embarque (Packing Type): MATERIAL\\\\nDestino (Ship To):\\\\nHISENSE MONTERREY HOME APPLIANCE MANUFACTURING S. DE\\\\nR.L. DE C.V.\\\\nAVENIDA PUERTO GRANDE No. 1301-B OTRA NO ESPECIFICADA EN EL\\\\nCATALOGO SALINAS VICTORIA, NL 65500 MEX\\\\nIMMEX: 201-2021, RFC: HMH200908BX7\\\\nTel: Fax:\\\\nVinculación: SI EXISTE VINCULACION Y NO AFECTA\\\\nEL VALOR ADUANA\\\\nVehículo:\\\\nTransp. Placas MX/US (Carrier Plates): /\\\\nPedimento Importación:\\\\nClave: IN\\\\nComentarios MX/US (Comments): /\\\\nContenedores:\\\\nBultos (Bundles): 378\\\\nSello (Seal):,\\\\nChofer (Driver):\\\\nIncoterms: DAP\\\\nTipo Vehiculo (Vehicle Type):\\\\nRuta (Route):\\\\nNo. Económico (Seal): \\'\\\\nContenedor Tipo (Container Type):\\\\nContenedor Placas (Container Plates) :\\\\nAg. Aduanal MX (Broker MX): WOODWARD (3977)\\\\nAg. Aduanal US (Broker USA): WOODWARD\\\\nPeso Neto\\\\n(Net Wt.)\\\\nLBS\\\\nCantidad(Qty)\\\\nFracción (HTS)\\\\nKG.\\\\nUM\\\\nCantidad(Qty)\\\\nUMC\\\\nNo. Parte\\\\nDescripción Español (Spanish Description)\\\\nV. Unit.\\\\nCompra\\\\nValor Tot.\\\\n(USD)\\\\n(Part No.)\\\\nDescripción Inglés (English Description), Orden de compra\\\\nOrigen\\\\n(Origin)\\\\ny Comercial\\\\nEmpaque\\\\n330.693 Lb\\\\n150.000 Kg\\\\n(P.O). Proveedor(Supplier), RFC(IRS), Num Entrada (Ent.\\\\nWH) Observaciones\\\\n(V. & Purch.\\\\ncommercial Unit)\\\\nM.N.\\\\n(Total Value)\\\\n1,500.000 PCS\\\\n1,500.000 PCS\\\\n1.1946545\\\\n8418999900 Tasa General:0%/Exp.USA:\\\\nPARTE PARA REFRIGERADOR\\\\n0.22072\\\\nCHN\\\\n0.22072\\\\n331.07\\\\n331.07\\\\nAir supply pipe component,\\\\n92.594 Lb\\\\n42.000 Kg\\\\n150.000 PCS\\\\n150.000 PCS\\\\n2.2143116\\\\n8544429903 Tasa General:5%/Exp.USA:\\\\n3.21182\\\\n481.77\\\\nCABLE ELECTRICOS CON CONEXIÓN\\\\nCHN\\\\n3.21182\\\\n481.77\\\\nBuilt-in line components,\\\\n321.654 Lb\\\\n145.900 Kg\\\\n500.000 PCS 3.2272063\\\\n500.000 PCS\\\\n8544429903 Tasa General:5% /Exp.USA:\\\\nARNES ELECTRICO\\\\nCHN\\\\n3.22162\\\\n3.22162\\\\n1,610.80\\\\n1,610.80\\\\nARNES ELECTRICO PARA ELETRODOMESTICO,\\\\n456.356 Lb\\\\n207.000 Kg\\\\n300.000 PCS\\\\n300.000 PCS\\\\n4.2306495\\\\n8544429903 Tasa General:5% /Exp.USA:\\\\ncables con conexiones\\\\nBuilt-in line components,\\\\nCHN\\\\n7.77832\\\\n7.77832\\\\n2,333.49\\\\n2,333.49\\\\n873.030 Lb\\\\n396.000 Kg\\\\n1,500.000 PCS\\\\n1,500.000 PCS\\\\n5.1961255\\\\n8418999900 Tasa General:0%/Exp.USA:\\\\npartes para refrigerador\\\\n0.39032\\\\n585.47\\\\nCHN\\\\n0.39032\\\\n585.47\\\\nCabinet Bottom Upper Plate,\\\\n6.503 Lb\\\\n2.950 Kg\\\\n600.000 PCS\\\\n600.000 PCS\\\\n6.4270149\\\\n3919100100 Tasa General:0% /Exp.USA:\\\\n0.06092\\\\n36.55\\\\nETIQUETA DE PLASTICO\\\\nCHN\\\\n0.06092\\\\n36.55\\\\n12.103 Lb\\\\n5.490 Kg\\\\n2,000.000 PCS\\\\n2,000.000 PCS\\\\n7.4287075\\\\nPara usos aduanales unicamente (For Custom Purposes Only)\\\\nRep. Legal:\\\\n3919100100 Tasa General:0% /Exp.USA:\\\\nETIQUETA AUTOADHESIVA\\\\n0.04132\\\\nCHN\\\\n0.04132\\\\nUsuario: LAURA PAOLA HERNANDEZ\\\\nAutorizo:\\\\n82.63\\\\n82.63\\\\nPages: 1 of 5\\\\nFecha Impresión: 10/28/2024 11:21:45 AM\\\\nFactImpBiSinAgrupacion.fr3\\\\nSEER-Tráfico\\\\n\", \\'FACTURA IMPORTACION BILINGÜE (Clave:IN)\\\\n(BILINGUAL IMPORT INVOICE (Code:IN))\\\\nCantidad(Qty)\\\\nPeso Neto\\\\n(Net Wt.)\\\\nUM\\\\nLBS\\\\nKG.\\\\nCantidad(Qty)\\\\nUMC\\\\nEmpaque\\\\nNo. Parte\\\\n(Part No.)\\\\nFactura (Invoice): 1598435H-4\\\\nFracción (HTS)\\\\nDescripción Español (Spanish Description)\\\\nDescripción Inglés (English Description), Orden de compra\\\\n(P.O). Proveedor(Supplier), RFC(IRS), Num Entrada (Ent.\\\\nWH) Observaciones\\\\nOrigen\\\\n(Origin)\\\\nV. Unit.\\\\nCompra\\\\ny Comercial\\\\n(V. & Purch.\\\\ncommercial Unit)\\\\nValor Tot.\\\\n(USD)\\\\nM.N.\\\\n(Total Value)\\\\nETIQUETA AUTOADHESIVA,\\\\n5.754 Lb\\\\n2.610 Kg\\\\n1,500.000 PCS 8.4348578\\\\n4821100100 Tasa General:0%/Exp.USA:\\\\n1,500.000 PCS\\\\nETIQUETAS IMPRESAS\\\\nCHN\\\\n0.07072\\\\n0.07072\\\\n106.07\\\\n106.07\\\\nCircuit diagram nameplate,\\\\n7,450.743 Lb\\\\n3,379.600 Kg\\\\n2,800.000 PCS\\\\n2,800.000 PCS\\\\n9.2018204\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n1.76672\\\\n4,946.81\\\\nCONDENSADOR\\\\nCHN\\\\n1.76672\\\\n4,946.81\\\\nCONDENSER,\\\\n570.556 Lb\\\\n258.800 Kg\\\\n200.000 PCS\\\\n200.000 PCS\\\\n10. 2164981\\\\n8418990300 Tasa General:0% /Exp.USA:\\\\n1.87602\\\\n375.20\\\\nCONDENSADOR\\\\nCHN\\\\n1.87602\\\\n375.20\\\\nCONDENSER,\\\\n31.944 Lb\\\\n14.490 Kg\\\\n315.000 PCS\\\\n315.000 PCS\\\\n11. 1980408\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.19132\\\\n60.26\\\\npartes para refrigerador\\\\nCHN\\\\n0.19132\\\\n60.26\\\\nConnection cavity parts,\\\\n292.859 Lb\\\\n132.839 Kg\\\\n612.000 PCS\\\\n612.000 PCS\\\\n12.1951784\\\\n8538100100 Tasa General:0% /Exp.USA:\\\\n0.62022\\\\n379.57\\\\nCARCASA PARA CAJA DE CONTROL\\\\nControl board box,\\\\nCHN\\\\n0.62022\\\\n379.57\\\\n104.940 Lb\\\\n47.600 Kg\\\\n1,400.000 PCS\\\\n1,400.000 PCS\\\\n13 2121396\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.40992\\\\n573.88\\\\npartes para refrigerador\\\\nCHN\\\\n0.40992\\\\n573.88\\\\nControl box cover parts,\\\\n123.458 Lb\\\\n56.000 Kg\\\\n800.000 PCS\\\\n800.000 PCS\\\\n14. 1999959\\\\n3923500100 Tasa General:15% /Exp.USA:\\\\n0.17172\\\\n137.37\\\\nTAPA DE PLASTICO\\\\nCHN\\\\n0.17172\\\\n137.37\\\\nControl panel box cover,\\\\n19.621 Lb\\\\n8.900 Kg\\\\n3,000.000 PCS\\\\n3,000.000 PCS\\\\n15.1959685\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.09182\\\\n275.45\\\\nPARTE PARA REFRIGERADOR\\\\nCopper-silver welding ring | Locke ring,\\\\nCHN\\\\n0.09182\\\\n275.45\\\\n28.571 Lb\\\\n12.960 Kg\\\\n60.000 PCS\\\\n60.000 PCS\\\\n16 2020593\\\\n8418999900 Tasa General:0%/Exp.USA:\\\\nPARTES PARA REFRIGERADOR\\\\nDoor upper end cover assembly,\\\\n0.63982\\\\n38.38\\\\nCHN\\\\n0.63982\\\\n38.38\\\\n74.516 Lb\\\\n33.800 Kg\\\\n148.000 PCS 17 2111790\\\\n148.000 PCS\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.68892\\\\n101.95\\\\nPARTES PARA REFRIGERADOR\\\\ndoor upper end cover,\\\\nCHN\\\\n0.68892\\\\n101.95\\\\n553.713 Lb\\\\n251.160 Kg\\\\n483.000 PCS 18. 1475698\\\\n483.000 PCS\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n1.75552\\\\n847.91\\\\ncarril de guia (parte de refrigerador)\\\\nDRAWER RAILS,\\\\nCHN\\\\n1.75552\\\\n847.91\\\\n949.222 Lb\\\\n430.560 Kg\\\\n828.000 PCS 19 1475699\\\\n828.000 PCS\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\ncarril de guia (parte de refrigerador)\\\\nDrawer rail parts,\\\\nCHN\\\\n1.75552\\\\n1.75552\\\\n1,453.56\\\\n1,453.56\\\\n949.222 Lb\\\\n430.560 Kg\\\\n828.000 PCS\\\\n828.000 PCS\\\\n20 1980225\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\nriel de guia (parte para refrigerador)\\\\nDrawer rail parts,\\\\n1.73592\\\\n1,437.34\\\\nCHN\\\\n1.73592\\\\n1,437.34\\\\n1,898.444 Lb\\\\n861.120 Kg\\\\n1,656.000 PCS\\\\n1,656.000 PCS\\\\n21. 1980223\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\nriel de guia (parte para refrigerador)\\\\nLEFT DRAWER RAILS,\\\\n1.73592\\\\n2,874.68\\\\nCHN\\\\n1.73592\\\\n2,874.68\\\\n7.142 Lb\\\\n3.240 Kg\\\\n60.000 PCS\\\\n60.000 PCS\\\\n22 1955932\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\npartes para refrigerador\\\\nCHN\\\\n0.59922\\\\n0.59922\\\\n35.95\\\\n35.95\\\\nDrawer trim,\\\\n30.930 Lb\\\\n14.030 Kg\\\\n300.000 PCS\\\\n300.000 PCS\\\\n23.1476034\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.78982\\\\nPARTE PARA REFRIGERADOR\\\\nDrawer trim,\\\\nCHN\\\\n0.78982\\\\n236.94\\\\n236.94\\\\n283.953 Lb\\\\n128.799 Kg\\\\n490.000 PCS\\\\n490.000 PCS\\\\n24.1944645\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\nPARTE PARA REFRIGERADOR\\\\nEvaporating dish,\\\\nCHN\\\\n0.44082\\\\n0.44082\\\\n216.00\\\\n216.00\\\\nPara usos aduanales unicamente (For Custom Purposes Only)\\\\nRep. Legal:\\\\nUsuario: LAURA PAOLA HERNANDEZ\\\\nAutorizo:\\\\nPages: 2 of 5\\\\nFecha Impresión: 10/28/2024 11:21:45 AM\\\\nFactImpBiSinAgrupacion.fr3\\\\nSEER-Tráfico\\\\n\\', \\'FACTURA IMPORTACION BILINGÜE (Clave:IN)\\\\n(BILINGUAL IMPORT INVOICE (Code:IN))\\\\nFactura (Invoice): 1598435H-4\\\\nPeso Neto\\\\n(Net Wt.)\\\\nCantidad(Qty)\\\\nUM\\\\nNo. Parte\\\\nLBS\\\\nKG.\\\\nEmpaque\\\\nCantidad(Qty)\\\\nUMC\\\\n(Part No.)\\\\nFracción (HTS)\\\\nDescripción Español (Spanish Description)\\\\nDescripción Inglés (English Description), Orden de compra\\\\nV. Unit.\\\\nCompra\\\\nOrigen\\\\n(Origin)\\\\ny Comercial\\\\nValor Tot.\\\\n(P.O). Proveedor(Supplier), RFC(IRS), Num Entrada(Ent.\\\\nWH) Observaciones\\\\n(V. & Purch.\\\\ncommercial Unit)\\\\n(USD)\\\\nM.N.\\\\n(Total Value)\\\\n322.227 Lb\\\\n146.160 Kg\\\\n444.000 PCS\\\\n444.000 PCS\\\\n25.1465357\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\nPARTES PARA REFRIGERADOR\\\\n1.07852\\\\nCHN\\\\n1.07852\\\\n478.86\\\\n478.86\\\\nEvaporating Dish,\\\\n162.698 Lb\\\\n73.799 Kg\\\\n340.000 PCS\\\\n340.000 PCS\\\\n26 2185010\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\n0.37072\\\\n126.04\\\\nPLATO EVAPORADOR (PARTE DE REFRIGERADOR)\\\\nCHN\\\\n0.37072\\\\n126.04\\\\nEvaporating dish\\\\\\\\B05082274|C01|PP\\\\\\\\BCD-516WY/HC1(H),\\\\n1.587 Lb\\\\n0.720 Kg\\\\n1,500.000 PCS 27.4121848\\\\n1,500.000 PCS\\\\n3920629999 Tasa General:25% /Exp.USA:\\\\nETIQUETAS DE PLASTICO\\\\n0.04132\\\\n61.97\\\\nCHN\\\\n0.04132\\\\n61.97\\\\nFilter guide label,\\\\n13.624 Lb\\\\n6.180 Kg\\\\n3,000.000 PCS\\\\n3,000.000 PCS 28 1508491\\\\n8418999900 Tasa General:0% /Exp.USA:\\\\npartes para refrigerador\\\\n0.04132\\\\n123.95\\\\nCHN\\\\n0.04132\\\\n123.95\\\\nFixed seat,\\\\n3.527 Lb\\\\n1.600 Kg\\\\n1,000.000 PCS\\\\n1,000.000 PCS\\\\n29 2192798\\\\n3810100100 Tasa PPS:0% Compl.: IIa/Exp. USA:\\\\n0.19132\\\\n191.31\\\\nFUNDENTE para soldar\\\\nCHN\\\\n0.19132\\\\n191.31\\\\nFlux Flux-Sealing Liquid \\\\\\\\LOKPREP 61S,\\\\n2,882.630 Lb\\\\n1,307.539 Kg\\\\n2,092.000 PCS 30.2160562\\\\n2,092.000 PCS\\\\n8302429199 Tasa General:15% /Exp.USA:\\\\nJALADERA PARA REFRIGERADOR\\\\nCHN\\\\n5.30442\\\\n5.30442\\\\n11,096.',\n",
       "   'expected': '{\"invoice\": [[0, 1, 2, 3, 4]]}',\n",
       "   'prediction': '{\"invoice\": [[0, 1, 2, 3, 4, 5, 6]], \"bill_of_lading\": [[7]], \"mill_certificate\": [[8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,'},\n",
       "  {'index': 135,\n",
       "   'input': 'You are an expert in identifying document types using OCR text. Your role is to look at the OCR texts inputs provided by the user and classify them into different document types. \\nYou will return the response in Output format defined below.\\n\\nThe possible document types are: [\\'mill_certificate\\', \\'invoice\\', \\'bill_of_lading\\', \\'packing_list\\'].\\n\\nExample:\\nInput: \\'{\\'ocr\\': [\"page 1 text\", \"page 2 text\"], \\'num_pages\\': 2}\\'\\nOutput: \\'{\"others\": [[1]], \"invoice\": [[0]]}\\'\\n\\nInput: {\\'ocr\\' : [\\'INDUX\\\\nIND850301QIO\\\\nLugar de expedición: Ciudad de México\\\\nRégimen Fiscal:General de Ley Personas Morales\\\\nLaminadora No.37, Col. Bellavista. Deleg. Alvaro Obregon. C.P.01140, Ciudad de México\\\\n011 52 (55) 5272 2099 / 2282 9330 / 01800 004 6389\\\\nE-mail:ventas@grupomarmex.mx/credito@grupomarmex.mx\\\\nPágina Web:www.indux.com.mx\\\\nVENDIDO A:XEXX010101000\\\\nBAIER & MICHELS GMBH & CO.KG\\\\nCARL SCHNEIDER STR.1\\\\nOBER-RAMSTADT/ROHRBACH, HESSE,64372\\\\nTAX ID:76006/00099\\\\nC.P.64372\\\\nCliente No.:\\\\nFecha:\\\\nVersión:\\\\nFecha Certificación:\\\\nSERIE:DF FACTURA:141851\\\\nNum. Certificado SAT:\\\\nNo. Certificado\\\\n12039903\\\\n2023-03-30\\\\n4.0\\\\n2023-03-30T10:30:13\\\\n00001000000505619865\\\\n00001000000505496946\\\\nFolio Fiscal UUID:316917A3-0D9A-45B4-BBDC-00605619DABC\\\\nExportación:(02)Definitiva\\\\n(Régimen Fiscal:616) Si sus facturas no son pagadas en el plazo\\\\nAlemania\\\\nEX\\\\nCONSIGNADO A:\\\\nBAIER & MICHELS GMBH & CO.KG\\\\nCARL SCHNEIDER STR.1\\\\nOBER-RAMSTADT/ROHRBACH, HESSE,64372\\\\nTAX ID:76006/00099\\\\nSU PEDIDO NO:\\\\nQUOTE # 22-1213\\\\nAlemania\\\\nconvenido causaran intereses moratorios del 3%\\\\nmensual\\\\nNo se aceptan rechazos ni devoluciones después\\\\nde 30 días hábiles posteriores a la fecha de\\\\nrecepción de los materiales\\\\nNota: En caso de que se trate de material de\\\\nfabricación especial, no se aceptaran devoluciones.\\\\nLa venta de los productos aquí señalados está sujeta a nuestros Términos y\\\\nCondiciones Comerciales disponibles en:\\\\nhttps://www.grupomarmex.mx/condiciones-comerciales-grupo-marmex\\\\nLa recepción de los productos se considera como la aceptación de todos los\\\\ntérminos establecidos.\\\\nUso CFDI:S01:Sin efectos fiscales.\\\\nVÍA DE EMBARQUE:\\\\nNUESTRO PEDIDO:\\\\nCO2786284\\\\nVENDEDOR:\\\\n23\\\\nCONDICIONES DE PAGO:0 PAGO\\\\nANTICIPADO\\\\nCLAVE PRODUCTO\\\\nCLAVE SAT\\\\nACTIVOS FIJOS-1\\\\n23281700\\\\nDESCRIPCIÓN\\\\nDESCRIPCIÓN SAT\\\\nMAQ. USADA SACK & KIESSELBACH\\\\nMáquinas para tratamiento mecánico de\\\\nsuperficie\\\\nUsed Sack & Kiesselbach Hydraulic Press.\\\\nSeries Nr. 27660-Year of Mfg 1963\\\\n315 Ton.\\\\nIncludes Hydraulic Pump model C1/Z630TC.\\\\nSeries Nr. 50593-Year of Mfg 1964\\\\nMounted on an iron plate 40 mm x 900 mm x\\\\n1700 mm.\\\\nINCOTERM: EXW-INDUX, S.A.\\\\nDE C.V. MÉXICO CITY.\\\\nUUID CFDI RELACIONADO\\\\nDEA0163E-8C1C-43AF-A6A9-0636FA2D2583 (Tipo Relacion:07)\\\\nUNIDAD\\\\nCANTIDAD\\\\nPZ\\\\n1.0\\\\nOBJ.\\\\nIMPUESTO\\\\n02\\\\nPRECIO\\\\nUNITARIO\\\\nIMPORTE\\\\n5,000.0000\\\\n5,000.00\\\\nSub-total:\\\\n5,000.00\\\\nIVA:\\\\nTotal:\\\\n0.00\\\\n5,000.00\\\\nIMPORTE CON LETRA: CINCO MIL DÓLARES AMÉRICANOS 00/100 USD\\\\nEl importe de esta factura deberá liquidarse en Dólares Americanos o su equivalente en pesos al tipo de cambio vigente al dia de su pago\\\\nCadena Original SAT:\\\\n||1.1|316917A3-0D9A-45B4-BBDC-00605619DABC|2023-03-\\\\n5CxvGISQB7CQeFLRSWRLxWLUMAO8z1Wv0TJ05pNDvYb/uJpYM5G80UhCMFyuT9sqCYB60NhWycU6fkZWC0F3dZN3sPkoWSjPHmYIIMW6yn7BzzXkcwSEGkOnfMheoVtqwvTOI+OTHV\\\\nMetodo de\\\\n30T10:30:13|SFE0807172W8|J+k4kbMSftblWSaGk+UqjleFYjay Biy6h65DV/OCOISGFHrHBvg Rwx0Sju2Xzyx3yD1aTyxahe5RCmPnj01ealmys RslDBdWdjpk691qkG7HWJXOgtLESQIC+Ti5G PUE:Pago en una sola\\\\nexhibición\\\\nForma de Pago:\\\\nLsQj3LIPxrE7g30P/7HM0g Rk1rJJK50ckzCCa919fDmZEX2qcL551+3GGLu/PzcNiWRYnPeDx55+G3A==|00001000000505619865||\\\\nSello Digital SAT:\\\\nLUMBEXF5+GG6iHmCnKcBbF6PSIGcfxq7NqTeSDRSz58/qh6ZHWbDDJozUWtuHu BrdjeA/AEBHszIRh9wh9XmXr9OwDV713hF6KfhvWdlmv1 lj9fcvSuf40RNYXyn6kTj9jaf0pvfm3nRhffjLUFxx\\\\nSWXUthHF88TLL3eYjuZ19GODB3+9/+LbaiAOyzKG14Hq0CYMOCc3zE2UU/ms4blAyYMBT MXarha Pxlvs8bFZw4TzDEBszsDDvQz9+V6D1r0sicbSMLvCmSbcwJr7gal6obmAXtvsU1dfmINV 03:Transferencia electrónica de\\\\nXT/eqljqmZFpK4RtMzAZJRXUMO1jC2vgOPpaW5S4DLpmjPg==\\\\nSello Digital Emisor:\\\\nJ+k4kbMSftblWSaGk+UqjleFYjqyBiy6h65DV/OCO+SGFHrHBvg Rwx0Sju2Xzyx3yD1aTyxahe5RCmPnj01ealmysRsIDBdWdjpk691qkG7HWJxOgtLESQfC+Ti5\\\\nG5CxvGISQB7CQeFLRSWRLXWLUMAO8z1Wv0TJ05pNDvYb/uJpYM5G80UhCMFyuT9sqCYB60NhWycU6fkZWC0F3dZN3sPkoWSjPHmYIIMW6yn7BzzXk\\\\ncwSEGK0nfMheoVtqwvTOI+OTHVLsQj3LIPxrE7g30P/7HM0gRk1rJJK50ckzCCa919fDmZEX2qcL551+3GGLu/PzcNiWRYnPeDx55+G3A==\\\\nfondos\\\\nMoneda:\\\\nUSD:Dolar americano\\\\nPagar a través de:\\\\nPago en pesos\\\\nBBVA Bancomer, S.A.\\\\nSucursal: 1817\\\\nCuenta: 0444070771\\\\nPara Transferencia Electronica\\\\nindicar su nombre y su compañía\\\\nCLABE: 012180004440707712\\\\nConvenio CIE: 008226\\\\nRef:12039903\\\\nBanco Nacional de México, S.A.\\\\nSucursal: 268\\\\nCuenta: 7176718\\\\nMéxico, CDMX\\\\nInsurgentes Félix Cuevas\\\\nCLABE: 002180026871767186\\\\n*CUENTA EN USD*\\\\nBBVA Bancomer, S.A.\\\\nBBVA Bancomer, S.A.\\\\nCuenta: 0444070798\\\\nABA: 122035487\\\\nSWIFT: BCMRMXMM\\\\nCLABE: 012180004440707987\\\\nPagaré:\\\\nDebemos y pagaremos incondicionalmente a la orden de INDUX la cantidad de\\\\nCINCO MIL DÓLARES AMÉRICANOS 00/100 USD en el domicilio de INDUX en la\\\\nCiudad de México correspondiente al valor de las mercancías recibidas a mi (nuestra)\\\\nentera satisfacción\\\\nFirma Aceptación\\\\nMéxico CDMX a:2023-03-30\\\\nEste documento es una representación impresa de un CFDI\\\\nPágina:1 de 2\\\\n\\', \\'INDUX\\\\nINDUX\\\\nIND850301QIO\\\\nLugar de expedición: Ciudad de México\\\\nRégimen Fiscal:General de Ley Personas Morales\\\\nLaminadora No.37, Col. Bellavista. Deleg. Alvaro Obregon. C.P.01140, Ciudad de México\\\\n011 52 (55) 5272 2099 / 2282 9330 / 01800 004 6389\\\\nE-mail:ventas@grupomarmex.mx/credito@grupomarmex.mx\\\\nPágina Web:www.indux.com.mx\\\\nVENDIDO A:XEXX010101000\\\\nBAIER & MICHELS GMBH & CO.KG\\\\nCARL SCHNEIDER STR.1\\\\nOBER-RAMSTADT/ROHRBACH, HESSE,64372\\\\nTAX ID:76006/00099\\\\nC.P.64372\\\\nCliente No.:\\\\nFecha:\\\\nVersión:\\\\nFecha Certificación:\\\\nSERIE:DF FACTURA:141851\\\\nNum. Certificado SAT:\\\\nNo. Certificado\\\\n12039903\\\\n2023-03-30\\\\n4.0\\\\n2023-03-30T10:30:13\\\\n00001000000505619865\\\\n00001000000505496946\\\\nFolio Fiscal UUID:316917A3-0D9A-45B4-BBDC-00605619DABC\\\\nExportación:(02)Definitiva\\\\n(Régimen Fiscal:616) Si sus facturas no son pagadas en el plazo\\\\nAlemania\\\\nEX\\\\nCONSIGNADO A:\\\\nBAIER & MICHELS GMBH & CO.KG\\\\nCARL SCHNEIDER STR.1\\\\nOBER-RAMSTADT/ROHRBACH, HESSE,64372\\\\nTAX ID:76006/00099\\\\nAlemania\\\\nSU PEDIDO NO:\\\\nVÍA DE EMBARQUE:\\\\nQUOTE # 22-1213\\\\nconvenido causaran intereses moratorios del 3%\\\\nmensual\\\\nNo se aceptan rechazos ni devoluciones después\\\\nde 30 días hábiles posteriores a la fecha de\\\\nrecepción de los materiales\\\\nNota: En caso de que se trate de material de\\\\nfabricación especial, no se aceptaran devoluciones.\\\\nLa venta de los productos aquí señalados está sujeta a nuestros Términos y\\\\nCondiciones Comerciales disponibles en:\\\\nhttps://www.grupomarmex.mx/condiciones-comerciales-grupo-marmex\\\\nLa recepción de los productos se considera como la aceptación de todos los\\\\ntérminos establecidos.\\\\nUso CFDI:S01:Sin efectos fiscales.\\\\nNUESTRO PEDIDO:\\\\nCO2786284\\\\nVENDEDOR:\\\\n23\\\\nCONDICIONES DE PAGO:0 PAGO\\\\nANTICIPADO\\\\n|INCOTERM EXW\\\\nDETALLE DEL COMPLEMENTO\\\\nFRACCIÓN\\\\nMARCA\\\\nMODELO\\\\nSUBMODEL\\\\nARANCELARI\\\\nΟ\\\\n8462199900\\\\nSACK &\\\\nKIESSELBACH\\\\n1963\\\\n1963\\\\nNÚMERO DE UNIDAD\\\\nSERIE\\\\nADUANA\\\\n27660\\\\nVALOR UNITARIO\\\\n06\\\\n5000.00\\\\nCANTIDAD\\\\nADUANA\\\\n1.000\\\\nVALOR EN\\\\nDÓLARES\\\\n5,000.00\\\\nIMPORTE CON LETRA: CINCO MIL DÓLARES AMÉRICANOS 00/100 USD\\\\nEl importe de esta factura deberá liquidarse en Dólares Americanos o su equivalente en pesos al tipo de cambio vigente al dia de su pago\\\\nCadena Original SAT:\\\\n||1.1|316917A3-0D9A-45B4-BBDC-00605619DABC|2023-03-\\\\nMetodo de\\\\n30T10:30:13|SFE0807172W8|J+k4kbMSftblWSaGk+UqjleFYjay Biy6h65DV/OCOISGFHrHBvg Rwx0Sju2Xzyx3yD1aTyxahe5RCmPnj01ealmys RslDBdWdjpk691qkG7HWJXOgtLESQIC+Ti5G PUE:Pago en una sola\\\\nexhibición\\\\n5CxvGISQB7CQeFLRSWRLxWLUMAO8z1Wv0TJ05pNDvYb/uJpYM5G80UhCMFyuT9sqCYB60NhWycU6fkZWC0F3dZN3sPkoWSjPHmYIIMW6yn7BzzXkcwSEGkOnfMheoVtqwvTOI+OTHV\\\\nLsQj3LIPxrE7g30P/7HM0g Rk1rJJK50ckzCCa919fDmZEX2qcL551+3GGLu/PzcNiWRYnPeDx55+G3A==|00001000000505619865||\\\\nSello Digital SAT:\\\\nLUMBEXF5+GG6iHmCnKcBbF6PSIGcfxq7NqTeSDRSz58/qh6ZHWbDDJozUWtuHu BrdjeA/AEBHszIRh9wh9XmXr9OwDV713hF6KfhvWdlmv1 lj9fcvSuf40RNYXyn6kTj9jaf0pvfm3nRhffjLUFxx\\\\nForma de Pago:\\\\nSWXUthHF88TLL3eYjuZ19GODB3+9/+LbaiAOyzKG14Hq0CYMOCc3zE2UU/ms4blAyYMBT MXarha Pxlvs8bFZw4TzDEBszsDDvQz9+V6D1r0sicbSMLvCmSbcwJr7gal6obmAXtvsU1dfmINV 03:Transferencia electrónica de\\\\nXT/eqljqmZFpK4RtMzAZJRXUMO1jC2vgOPpaW5S4DLpmjPg==\\\\nSello Digital Emisor:\\\\nJ+k4kbMSftblWSaGk+UqjleFYjqyBiy6h65DV/OCO+SGFHrHBvg Rwx0Sju2Xzyx3yD1aTyxahe5RCmPnj01ealmysRsIDBdWdjpk691qkG7HWJxOgtLESQfC+Ti5\\\\nG5CxvGISQB7CQeFLRSWRLXWLUMAO8z1Wv0TJ05pNDvYb/uJpYM5G80UhCMFyuT9sqCYB60NhWycU6fkZWC0F3dZN3sPkoWSjPHmYIIMW6yn7BzzXk\\\\ncwSEGK0nfMheoVtqwvTOI+OTHVLsQj3LIPxrE7g30P/7HM0gRk1rJJK50ckzCCa919fDmZEX2qcL551+3GGLu/PzcNiWRYnPeDx55+G3A==\\\\nfondos\\\\nMoneda:\\\\nUSD:Dolar americano\\\\nPagar a través de:\\\\nPago en pesos\\\\nBBVA Bancomer, S.A.\\\\nSucursal: 1817\\\\nCuenta: 0444070771\\\\nPara Transferencia Electronica\\\\nindicar su nombre y su compañía\\\\nCLABE: 012180004440707712\\\\nConvenio CIE: 008226\\\\nRef:120399',\n",
       "   'expected': '{\"invoice\": [[0, 1]]}',\n",
       "   'prediction': '{\"invoice\": [[0, 1]], \"bill_of_lading\": [[3]], \"mill_certificate\": [[10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], ['}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_combined_evaluation(val_dataset, wrapped_predict_fn)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007b4dd-adb8-40f5-9096-a374db106888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907c5cb-6a2e-4163-b21e-b78e56095741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4de1c-8dc7-4f4e-a8f8-46591da6602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DOCUMENT CLASSIFICATION EVALUATION RESULTS =====\n",
      "\n",
      "----- Document-Level Metrics -----\n",
      "Number of documents: 1\n",
      "Exact match rate: 0.0000\n",
      "Partial match rate: 0.0000\n",
      "Macro Precision: 0.3333\n",
      "Macro Recall: 0.3333\n",
      "Macro F1: 0.3333\n",
      "\n",
      "----- Class-Level Metrics -----\n",
      "Class                 Precision     Recall         F1    Support\n",
      "-----------------------------------------------------------------\n",
      "packing_list             0.0000     0.0000     0.0000          0\n",
      "invoice                  1.0000     1.0000     1.0000          1\n",
      "bill_of_lading           0.0000     0.0000     0.0000          1\n",
      "\n",
      "----- Macro Averages -----\n",
      "Macro Precision: 0.3333\n",
      "Macro Recall: 0.3333\n",
      "Macro F1: 0.3333\n",
      "\n",
      "===== END OF EVALUATION RESULTS =====\n",
      "\n",
      "\n",
      "📊 Document-Level Metrics:\n",
      " {'exact_match_rate': 0.0, 'partial_match_rate': 0.0, 'no_match_rate': 1.0, 'average_overlap': 0.3333333333333333, 'macro_precision': 0.3333333333333333, 'macro_recall': 0.3333333333333333, 'macro_f1': 0.3333333333333333, 'num_samples': 1}\n",
      "\n",
      "📦 Group-Level Metrics:\n",
      " {'level2_group_exact_match_rate': 1.0, 'level3_segmentation_recall': 0.5, 'over_segmentation_rate': 1.0, 'under_segmentation_rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "## somthings off doc-level prediction maybe? --check later\n",
    "\n",
    "# Sample prediction and ground truth\n",
    "pred_str = '{\"invoice\": [[0]], \"packing_list\": [[1]]}'\n",
    "true_str = '{\"invoice\": [[0]], \"bill_of_lading\": [[1]]}'\n",
    "\n",
    "from evaluator import DocumentEvaluator, evaluate_strict_document_groups\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = DocumentEvaluator()\n",
    "\n",
    "# Add single sample\n",
    "evaluator.add_sample(pred_str, true_str)\n",
    "\n",
    "# Run document-level evaluation\n",
    "doc_metrics = evaluator.evaluate()\n",
    "\n",
    "# Run group-level evaluation\n",
    "group_metrics = evaluate_strict_document_groups([pred_str], [true_str])\n",
    "\n",
    "# Print both\n",
    "print(\"\\nDocument-Level Metrics:\\n\", doc_metrics)\n",
    "print(\"\\nGroup-Level Metrics:\\n\", group_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abb130-079e-45c8-a48d-bb7da762483c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adf69e-4786-47f1-87fd-4769aee13761",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FOR FUTURE REFERENCE (CONTINUED FINETUNING)\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Step 1: Load base model (same as before)\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",  # Same as used earlier\n",
    "#     max_seq_length = 2048,\n",
    "#     dtype = None,\n",
    "#     load_in_4bit = True,  # or False depending on your setup\n",
    "# )\n",
    "\n",
    "# # Step 2: Load your previously trained LoRA weights\n",
    "# model.load_adapter(\"your-username/your-lora-repo-name\")\n",
    "\n",
    "# # Step 3: Prepare your dataset (old, new, or combined)\n",
    "# # dataset = your new or extended dataset\n",
    "\n",
    "# # Step 4: Fine-tune further using Unsloth's training loop\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 64,\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#     lora_alpha = 16,\n",
    "#     lora_dropout = 0,\n",
    "#     bias = \"none\",\n",
    "#     use_gradient_checkpointing = True,\n",
    "#     random_state = 42,\n",
    "#     use_rslora = False,\n",
    "#     loftq_config = None,\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
